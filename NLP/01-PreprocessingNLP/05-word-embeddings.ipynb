{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "## Why Embedding?\n",
    "As we know, machines can't handle text, it can only handle numbers. But how to convert a word to numbers?\n",
    "\n",
    "The most naive approach would be to take a list of all the words in your text and attribute a number to all of them. It will work but you can imagine that some problems will appear:\n",
    "* How do you handle unknown words? \n",
    "* If your text contains `doctor`, `nurse`, and `candy`. `doctor` and `nurse` have a strong similarity but `candy` doesn't. How can we make the machine understand that? With our naive technique, `doctor` could have the number `5` associated to it and nurse the number `98767`.\n",
    "\n",
    "Of course, a lot of people already spent some time with those problems. the solution that came out of it is \"Embedding\". \n",
    "\n",
    "## What is embeddings?\n",
    "\n",
    "An embedding is a **VECTOR** which represents a word or a document.\n",
    "\n",
    "A vector will be attributed to each token. Each vector will contain multiple dimensions (usually tens or hundreds of dimensions).\n",
    "\n",
    "```\n",
    "[...] associate with each word in the vocabulary a distributed word feature vector [...] The feature vector represents different aspects of the word: each word is associated with a point in a vector space. The number of features [...] is much smaller than the size of the vocabulary.\n",
    "```\n",
    "- [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), 2003.\n",
    "\n",
    "Long story short, embeddings convert words into vectors in a way that allows the machine to understand the similarity betweens them.\n",
    "\n",
    "Each embedding library has it's own way of classifying words, it will regroup words into big categories. Each word will get a score for each category.\n",
    "\n",
    "To take a simple example the word `mother` could be classified like that:\n",
    "\n",
    "|        | female | family | human | animal|\n",
    "|--------|--------|---------|-------|-------|\n",
    "| mother | 0.9    | 0.9.    | 0.7   | 0.1   |\n",
    "\n",
    "**Explanations:** Mother has a strong similarity with female, family and human but it has a low similarity with animal.\n",
    "\n",
    "**Disclaimer:** Those numbers and categories are totally arbitrary and are only here to show an example.\n",
    "\n",
    "Here is another example with more complete datas:\n",
    "\n",
    "![embedding](https://miro.medium.com/max/2598/1*sAJdxEsDjsPMioHyzlN3_A.png)\n",
    "\n",
    "## Should I do it by hand?\n",
    "\n",
    "You could, but if some people already did the job for you and spent a lot of time to optimize it, why not use it?\n",
    "\n",
    "## What to use?\n",
    "\n",
    "There are a lot of libraries out there for embeddings. Which one is the best? Once again, *it depends*. The results will change depending on the text you are using, the information you want to extract, the model you use,...\n",
    "\n",
    "Choosing the \"best\" embedding model will be part of the hyper-optimization that you can do at the end of a project.\n",
    "\n",
    "If you want understand embeddings more in depth, [follow this link](http://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "Here are some of the best libraries of the moment:\n",
    "\n",
    "* [Flair](https://github.com/flairNLP/flair) (University of Berlin)\n",
    "* [fasttext](https://fasttext.cc/) (Facebook)\n",
    "* [GloVe](https://github.com/stanfordnlp/GloVe) (Stanford)\n",
    "\n",
    "And the oldest way doing it (but still good):\n",
    "* [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice time!\n",
    "\n",
    "Enough reading, let's practice a bit. Can you use SpaCy to embed this sentence?\n",
    "Read the [spacy embedding documentation](https://spacy.io/usage/vectors-similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ -1.8607  ,   0.15804 ,  -4.1425  ,  -8.6359  , -16.955   ,\n",
      "         1.157   ,  -1.588   ,   5.6609  , -12.03    ,  16.417   ,\n",
      "         4.1907  ,   5.5122  ,  -0.11932 ,  -6.06    ,   3.8957  ,\n",
      "        -7.8212  ,   3.6736  , -14.824   ,  -7.6638  ,   2.5344  ,\n",
      "         7.9893  ,   3.6785  ,   4.3296  , -11.338   ,  -3.5506  ,\n",
      "        -5.899   ,   1.0998  ,   3.4515  ,  -5.4191  ,   1.8356  ,\n",
      "        -2.902   ,  -7.9294  ,  -1.1269  ,   8.4124  ,   5.1416  ,\n",
      "        -3.1489  ,  -4.2061  ,  -1.459   ,   7.8313  ,   0.27859 ,\n",
      "        -4.3832  ,   8.0756  ,  -0.94784 ,  -6.1214  ,   8.2792  ,\n",
      "         5.0529  ,  -8.3611  ,  -6.0743  ,  -0.53773 ,   2.7538  ,\n",
      "         3.8162  ,  -4.1612  ,   0.7591  ,  -2.8374  ,  -6.4851  ,\n",
      "        -3.3435  ,   3.2703  ,   2.759   ,   2.6645  ,   4.0013  ,\n",
      "        13.381   ,  -5.2907  ,  -3.133   ,   4.5374  , -11.899   ,\n",
      "        -6.716   ,  -0.041939,  -2.0879  ,   3.0101  ,  10.3     ,\n",
      "         2.6835  ,   2.7265  ,   8.3018  ,  -4.4563  ,  14.43    ,\n",
      "         3.9642  ,  -4.8287  ,  -5.648   ,  -7.2597  , -11.475   ,\n",
      "        -2.6171  ,   0.3325  ,  14.454   ,  -5.155   ,   0.93722 ,\n",
      "        -2.6187  ,  -1.783   ,   3.8711  ,   1.4681  ,  -6.705   ,\n",
      "        -4.0953  ,  -0.22536 ,   9.444   , -10.305   ,  -0.13202 ,\n",
      "        -2.5534  ,   0.36113 ,  -8.539   ,   2.6755  ,  -2.5872  ,\n",
      "         2.8679  ,   9.7515  ,  -2.1221  ,   0.82061 , -10.319   ,\n",
      "         1.1547  ,  -6.5808  ,   4.9236  ,  -5.0744  ,  -1.4781  ,\n",
      "        -4.9195  ,  -8.3767  ,   9.5575  ,  -3.685   ,   2.1198  ,\n",
      "        -0.17453 ,   2.0831  ,  -3.9546  ,   1.0606  ,   2.7273  ,\n",
      "        -3.8842  ,  -1.3651  ,  -9.3881  ,   8.7217  ,   0.36486 ,\n",
      "         6.9968  ,   5.9808  ,  -1.9575  ,  -4.9097  ,   0.17648 ,\n",
      "         0.82424 ,   3.5459  ,   0.63201 ,  -5.3636  ,   0.18387 ,\n",
      "         5.3658  ,  -7.0217  ,  -6.1577  ,  12.318   ,  -9.2032  ,\n",
      "        -3.8502  ,  -2.3115  ,   3.8475  ,   1.1169  , -19.406   ,\n",
      "         1.3987  ,   4.8752  ,  -1.3337  , -10.459   , -10.679   ,\n",
      "        -8.125   ,   1.8708  ,  -8.5214  ,  -5.3364  ,  -1.5602  ,\n",
      "         1.5305  ,   7.3319  ,   1.4623  ,  -4.2276  ,   3.925   ,\n",
      "        -3.6453  ,  -8.9947  ,  -9.9602  ,   4.5267  ,  -2.8874  ,\n",
      "         3.9509  ,   0.33015 ,  -0.69045 ,  -8.5682  ,   5.6035  ,\n",
      "         4.4303  ,  -0.12195 ,  -9.1766  ,   7.1214  ,  11.119   ,\n",
      "       -13.376   , -11.446   ,  -4.6233  ,   2.5693  ,   1.0571  ,\n",
      "        -3.7533  ,   2.1846  ,  11.634   ,  -0.71756 ,  -3.1931  ,\n",
      "         9.6407  ,  -0.97389 ,  -9.5713  ,   7.6816  ,   6.216   ,\n",
      "        -3.557   ,   8.6693  ,  -9.1699  ,   6.9589  ,  -3.7244  ,\n",
      "        -1.4941  , -11.266   ,   3.6515  ,  -6.93    ,  13.741   ,\n",
      "        -5.546   ,   1.3314  ,  -3.5116  ,   4.2548  ,  -4.1717  ,\n",
      "         0.6486  ,  13.1     ,  -5.2464  ,  -2.2528  ,  -4.0023  ,\n",
      "         0.80613 ,   0.41242 ,   5.1636  ,   6.016   , -11.018   ,\n",
      "         2.5725  ,  -5.4786  ,   3.9834  ,   1.7688  ,   1.0016  ,\n",
      "        -6.0102  ,   0.75538 ,  -3.0043  ,   5.4985  ,   5.7801  ,\n",
      "        -7.1877  ,  -0.80729 ,   3.1282  ,   0.39466 ,   2.0783  ,\n",
      "       -17.384   ,  -7.25    ,   3.3675  ,  -2.8671  ,  -4.7587  ,\n",
      "         2.5144  ,  -8.2825  ,  13.736   ,  -0.033851,  -6.3096  ,\n",
      "       -17.447   ,  -2.4643  ,   5.0504  ,   0.05487 ,   8.3954  ,\n",
      "        -7.3362  ,   6.2495  ,  -0.50043 ,  13.246   ,   4.4032  ,\n",
      "        -5.1087  ,   8.4952  , -11.692   ,   1.8684  ,  -4.0402  ,\n",
      "        -7.5522  ,  -3.1862  ,   1.7126  ,  -8.6044  ,   7.4476  ,\n",
      "         9.7846  ,   4.0875  ,   6.8338  ,   7.4497  ,   6.187   ,\n",
      "        -5.592   ,   4.6493  ,   0.78633 , -10.055   ,   5.5839  ,\n",
      "        -5.2267  ,  -3.3644  , -14.551   ,   2.5234  ,  -4.6496  ,\n",
      "        -5.5413  ,  -0.79268 ,  -9.7865  ,   2.4501  ,  -0.26537 ,\n",
      "       -15.947   , -10.645   ,   3.9164  ,   1.9886  ,   9.6709  ,\n",
      "        -1.5746  ,   1.333   ,   1.628   ,  -3.5716  ,   2.7336  ,\n",
      "         9.7687  ,   1.9344  ,   4.3976  ,  -2.1922  ,  -4.7053  ,\n",
      "         0.61891 ,   8.0963  ,   7.9379  , -11.57    ,   4.3334  ],\n",
      "      dtype=float32), array([ 2.0565e+00, -3.2259e+00, -5.7364e+00, -6.1460e+00,  1.5748e-01,\n",
      "       -2.4284e+00,  7.6580e+00,  2.7064e+00, -2.2110e+00, -8.9990e-01,\n",
      "        6.7584e+00, -2.6983e+00, -7.6898e+00,  2.4036e+00,  7.9365e+00,\n",
      "       -2.1374e+00, -1.7134e-01, -1.9848e+00,  2.1000e+00,  2.0230e+00,\n",
      "       -1.1329e-01,  3.7908e+00, -3.3405e+00, -8.5698e+00,  3.6204e+00,\n",
      "        9.6741e-01, -2.4264e+00,  4.4687e+00,  1.5334e+00,  1.3886e+00,\n",
      "        1.4789e+00, -4.5457e+00, -1.0838e+00, -1.9183e+00,  3.3245e+00,\n",
      "       -1.4215e-01, -1.9783e+00,  5.4134e-01,  1.9844e+00, -7.2322e-02,\n",
      "       -1.3614e+00,  3.2423e+00,  3.5776e+00, -2.6719e+00,  1.0900e+00,\n",
      "        4.6470e+00, -2.1616e+00, -3.8358e+00,  4.1603e+00,  4.6552e+00,\n",
      "       -1.6725e+00, -4.4985e+00, -2.8982e-01, -4.5826e+00, -4.6451e-02,\n",
      "       -4.6299e-01,  8.8783e-01,  1.2574e+00,  6.6601e+00,  5.5503e+00,\n",
      "        3.5401e+00,  2.6721e-01,  3.7113e+00, -4.0592e+00,  9.4553e-01,\n",
      "        5.9527e+00, -4.1922e+00, -6.9438e+00,  5.0857e+00,  1.4050e+00,\n",
      "       -2.4574e+00,  7.3218e+00, -2.1215e+00, -6.7814e-01, -2.2871e+00,\n",
      "        1.8938e+00, -2.5832e+00,  4.3815e+00,  4.2151e+00, -3.6183e+00,\n",
      "       -1.4703e+00, -1.3201e+00,  2.6590e+00, -5.6996e+00, -2.0867e-02,\n",
      "        9.6205e-01,  2.0081e+00,  1.4750e+00, -7.7691e+00, -3.4898e+00,\n",
      "        3.8443e+00,  3.2584e+00, -6.3080e-01, -8.3240e+00, -3.6071e+00,\n",
      "       -5.0766e+00,  1.5934e+00, -2.2644e+00, -1.5554e+00, -2.9191e+00,\n",
      "       -1.8064e-01,  6.5120e-01, -2.9984e-01,  4.0194e+00,  2.2026e-01,\n",
      "        8.0274e-01, -1.7585e+00,  1.8378e+00, -3.2169e+00,  2.6540e+00,\n",
      "        2.3664e+00, -6.5790e+00,  6.5348e-01, -1.3508e+00,  2.1475e+00,\n",
      "        3.2138e+00, -8.9947e+00, -7.6404e+00, -1.2826e+00, -1.0916e+00,\n",
      "       -4.8551e+00,  2.3193e+00, -5.1803e+00,  7.3055e-01, -1.5054e+00,\n",
      "        3.5798e-01,  4.8880e+00, -3.3836e+00,  8.9427e-01, -2.6204e+00,\n",
      "       -3.7773e+00,  2.2107e+00, -4.8519e+00, -9.8902e-01, -3.3669e+00,\n",
      "        2.7894e+00, -5.0841e+00, -5.7924e-01,  3.9221e+00, -2.8150e+00,\n",
      "       -1.3585e+00,  4.9767e+00,  4.6014e+00,  1.8252e+00, -9.3133e-01,\n",
      "       -4.7533e+00, -3.0548e+00,  3.4432e-01, -3.3965e+00, -2.8556e-01,\n",
      "        5.5976e-01, -2.4043e+00,  2.4901e-02, -1.7460e+00, -2.4686e+00,\n",
      "        5.2176e+00, -7.7651e-02,  4.1721e-01,  9.3170e-01, -3.6655e+00,\n",
      "        9.2885e-01, -5.1232e+00, -1.4109e+00, -2.2130e+00,  1.2593e-01,\n",
      "        1.7002e-01,  3.6620e-01, -3.3297e+00, -2.8706e+00, -3.5471e+00,\n",
      "        2.8726e+00, -1.9366e+00,  2.2992e+00, -1.2327e+00,  2.2837e+00,\n",
      "       -4.9106e+00,  3.7301e+00,  5.5283e+00,  4.2388e+00, -2.1416e+00,\n",
      "        3.8090e+00, -2.8421e+00,  4.9338e+00, -1.4347e+00, -1.7137e+00,\n",
      "        3.0704e+00,  3.1565e+00, -2.9751e+00,  2.4606e+00,  4.1388e+00,\n",
      "        4.0442e-01,  2.2151e+00, -1.1582e+00, -1.2870e+00,  1.3246e+00,\n",
      "       -3.9480e+00, -3.0005e+00,  3.3863e+00, -2.7484e+00, -1.3876e+00,\n",
      "        1.0230e+00, -2.6053e+00, -6.9901e+00, -2.2942e+00, -6.8935e+00,\n",
      "       -4.1983e+00,  9.8276e-01,  1.3395e+00, -4.8869e-02, -1.2287e+00,\n",
      "        1.4470e+00, -7.9510e-01,  4.5749e+00,  6.5947e-01,  2.2056e+00,\n",
      "        2.6567e+00, -4.8067e+00,  4.3957e+00, -1.2664e+00, -1.5321e+00,\n",
      "        4.6449e+00, -3.8600e-01,  4.4860e-01,  9.9003e+00, -1.8173e+00,\n",
      "        1.5570e+00, -2.0185e+00, -2.5939e-01,  7.0708e+00,  2.1668e+00,\n",
      "       -5.5965e+00, -3.6880e+00,  1.0096e+00, -5.1063e+00, -3.2259e-01,\n",
      "        3.9952e-01, -5.6467e+00,  5.2380e+00,  1.7802e-01, -2.3491e-01,\n",
      "       -6.9355e-01, -3.3943e+00,  1.0203e+00,  1.3879e+00,  3.7037e+00,\n",
      "       -1.1950e-01, -4.8030e+00, -3.3641e+00,  3.7098e+00,  3.9947e-01,\n",
      "       -3.6545e+00,  2.5716e+00, -1.3741e+00, -1.6315e+00,  2.0852e+00,\n",
      "        2.8567e-01, -2.0594e+00,  2.6534e+00, -6.3363e+00,  1.8357e+00,\n",
      "       -1.4042e+00, -3.6166e-01,  1.3117e+00, -9.2785e-01,  3.6175e+00,\n",
      "       -2.6949e+00, -2.2252e+00,  5.5984e-02, -3.1778e+00,  4.1249e-01,\n",
      "        6.4716e+00, -1.3846e+00, -2.0004e+00, -4.4885e-01, -3.6592e+00,\n",
      "        2.4708e+00,  6.9389e+00, -4.7911e+00, -1.1108e+00,  4.3447e+00,\n",
      "       -3.4196e+00, -7.2039e-01,  2.5732e+00, -3.7553e-01, -1.4460e+00,\n",
      "       -6.7010e-01,  1.0171e+00,  2.8546e+00, -4.2695e+00,  1.4214e+00,\n",
      "        1.5802e+00,  1.7597e+00, -6.0806e-01, -6.6107e+00,  9.3832e-03,\n",
      "       -4.2763e+00, -5.0507e-01,  5.0049e+00, -8.5312e+00, -1.4967e+00],\n",
      "      dtype=float32), array([ 5.2415e-01, -1.1843e-01,  9.9918e-01, -9.2592e-01,  5.8506e-01,\n",
      "        2.0744e+00,  2.6959e+00,  2.2769e+00, -4.9479e+00, -7.9734e-01,\n",
      "        9.2866e+00,  3.6112e+00, -5.2253e+00,  3.4473e+00,  1.3158e+00,\n",
      "       -4.6086e-01,  4.7559e+00, -1.9305e-01, -3.8121e+00,  4.2875e+00,\n",
      "       -3.2872e-01,  1.8281e+00, -1.4754e+00, -1.9032e+00, -1.7032e+00,\n",
      "       -4.0926e+00, -1.8351e+00,  1.0870e+00, -3.3411e+00,  7.4851e+00,\n",
      "        1.1742e+00, -3.0373e-01, -2.5485e+00, -1.9279e+00,  1.6646e+00,\n",
      "       -1.0174e+00,  2.3636e+00,  4.4008e-01,  4.0041e+00, -3.6133e+00,\n",
      "        7.6183e-01,  2.2741e+00, -1.4279e+00, -8.0863e-01, -3.6204e+00,\n",
      "        2.2990e+00, -6.2229e-01, -3.9037e+00,  2.9872e+00, -6.3003e-01,\n",
      "        2.3043e+00,  6.9002e-01, -1.2194e+00, -3.3751e+00, -1.3489e+00,\n",
      "        3.4724e+00, -1.3294e+00,  1.4738e+00,  2.9479e+00, -4.7675e+00,\n",
      "        3.6388e+00,  3.5650e+00,  1.4891e+00,  1.1954e+00,  3.3699e+00,\n",
      "        5.7875e+00, -4.6507e+00, -4.6967e+00,  2.6579e+00,  1.8115e+00,\n",
      "       -3.1754e+00,  1.1248e+00, -2.2645e+00,  1.1069e+00,  1.8203e-01,\n",
      "       -2.2554e-01, -7.2056e+00,  1.5426e+00, -3.3017e+00, -4.0340e-01,\n",
      "       -9.1280e+00, -5.8183e-01, -5.0528e-01,  2.3604e+00, -1.2982e+00,\n",
      "        3.5190e+00, -9.1207e-01, -5.2398e+00,  8.0220e-01, -1.9270e+00,\n",
      "        7.9124e-01,  2.0268e+00,  4.1546e-01, -5.6516e+00, -2.8672e+00,\n",
      "        1.6159e+00, -1.9610e+00, -1.3406e+00,  3.9821e+00,  2.1241e+00,\n",
      "        3.4758e+00,  1.6072e+00,  6.4524e+00,  4.7497e+00, -1.8914e+00,\n",
      "        4.8812e+00,  3.9606e-01, -2.8332e+00, -2.3677e+00,  4.6346e-02,\n",
      "        4.7779e+00, -2.1437e+00, -5.7385e+00,  7.9065e-01,  2.5260e+00,\n",
      "        2.3132e+00, -5.7583e+00, -4.3681e-01, -2.5120e+00, -4.5710e+00,\n",
      "       -7.2244e+00,  8.9461e-01, -1.5163e+00, -1.5839e+00,  1.0655e+00,\n",
      "       -3.7839e+00,  2.3238e+00, -5.2761e+00,  3.5663e+00, -4.4710e+00,\n",
      "       -4.1136e+00,  3.5428e-01,  4.7834e+00,  3.2816e-01, -1.1519e+00,\n",
      "       -9.2081e-03, -6.5620e+00,  9.0176e-01,  5.4392e+00, -3.5219e-01,\n",
      "        3.3274e+00,  1.6719e+00, -2.8215e+00,  1.0678e+00,  1.7774e+00,\n",
      "       -1.7107e+00, -6.0280e+00, -2.9113e+00, -2.4965e-01,  2.4730e+00,\n",
      "        3.5397e-01,  2.2963e+00,  3.5072e+00, -8.0545e-01, -2.0138e+00,\n",
      "        9.0772e-01,  3.8639e+00,  4.7234e+00,  7.2134e-01, -7.7882e+00,\n",
      "       -1.7548e+00, -3.5279e-01, -2.1562e-01,  1.5422e+00, -1.9912e+00,\n",
      "       -3.6084e+00,  3.5919e-01,  1.5846e+00, -1.2086e+00, -3.3831e-01,\n",
      "       -7.9229e-01,  1.2602e+00,  1.9991e-01,  2.0949e+00,  7.3416e+00,\n",
      "        2.0928e+00,  4.4233e+00, -3.7646e+00, -2.8372e+00, -4.6271e+00,\n",
      "       -1.1910e+00,  1.9580e+00,  6.1960e+00, -2.6755e+00, -2.5112e+00,\n",
      "        2.1277e+00, -4.9013e-01, -6.3878e+00,  1.1589e+00,  1.4311e+00,\n",
      "       -1.3822e+00, -2.1891e+00, -5.0577e-01, -1.5217e+00,  3.7201e+00,\n",
      "       -4.8566e+00, -4.0714e+00, -4.7532e-01, -6.0586e-01, -2.4879e-01,\n",
      "        2.0676e-01, -8.7559e-03, -2.4218e+00, -4.1957e+00,  5.0531e+00,\n",
      "       -1.8014e+00, -4.9046e+00,  1.9647e+00, -3.1957e-01, -2.2874e+00,\n",
      "        4.1063e+00,  3.5684e-02,  4.6252e-01,  1.2987e+00, -4.6031e+00,\n",
      "        2.0689e+00,  3.3452e+00,  2.7237e-02, -6.0581e-01,  2.5498e+00,\n",
      "       -1.8716e+00,  3.7908e+00, -3.2418e+00,  2.2543e+00,  2.5722e+00,\n",
      "       -1.9206e-01,  1.7479e+00, -1.2751e+00,  4.1059e+00,  3.6211e+00,\n",
      "        6.3604e+00, -2.9154e+00, -2.0393e+00,  1.4650e+00,  1.4673e+00,\n",
      "        1.4110e+00,  1.2549e+00, -1.0626e+00, -4.9468e-01,  2.5788e+00,\n",
      "       -6.7368e+00, -1.2697e+00, -1.4319e+00,  2.0562e+00,  1.5537e+00,\n",
      "        5.1767e+00, -4.7995e+00, -8.8018e-01,  1.2586e-01,  6.3989e-01,\n",
      "       -1.9193e+00,  4.5036e+00, -4.7457e+00,  1.0940e+00, -3.0832e+00,\n",
      "        7.1560e-01,  3.2145e+00, -3.7101e+00, -6.9132e-02,  2.6534e+00,\n",
      "       -2.3662e+00,  6.8079e+00,  6.3903e+00, -2.8253e-01,  1.7868e+00,\n",
      "       -1.8786e+00, -2.6506e+00,  2.0292e+00, -3.0831e+00, -2.5823e+00,\n",
      "        1.5028e-01, -3.8987e+00,  1.2872e+00, -4.5131e+00, -2.4275e+00,\n",
      "       -1.0103e+00,  7.7083e-01, -3.4718e+00, -6.1662e-01,  9.6295e-01,\n",
      "        2.8688e+00, -8.8213e-01,  2.8095e+00,  3.6612e-01, -8.3534e-01,\n",
      "        2.9433e-01,  4.6173e+00,  2.8232e+00, -2.3742e+00, -1.8487e+00,\n",
      "        9.8842e-01, -1.1012e+00, -1.5634e+00, -3.1777e+00,  1.6411e+00,\n",
      "       -4.3231e+00,  1.3725e+00, -1.7378e+00, -4.2054e+00,  4.2028e+00],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "sentence = \"I love learning\"\n",
    "tokens = nlp(sentence)\n",
    "\n",
    "tokens_vectored_list = []\n",
    "for token in tokens:\n",
    "    token_vectored = token.vector\n",
    "    tokens_vectored_list.append(token_vectored)\n",
    "print(tokens_vectored_list)\n",
    "\n",
    "# Embed `sentence` with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of each word's vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: I, Vector: [ -1.8607     0.15804   -4.1425    -8.6359   -16.955      1.157\n",
      "  -1.588      5.6609   -12.03      16.417      4.1907     5.5122\n",
      "  -0.11932   -6.06       3.8957    -7.8212     3.6736   -14.824\n",
      "  -7.6638     2.5344     7.9893     3.6785     4.3296   -11.338\n",
      "  -3.5506    -5.899      1.0998     3.4515    -5.4191     1.8356\n",
      "  -2.902     -7.9294    -1.1269     8.4124     5.1416    -3.1489\n",
      "  -4.2061    -1.459      7.8313     0.27859   -4.3832     8.0756\n",
      "  -0.94784   -6.1214     8.2792     5.0529    -8.3611    -6.0743\n",
      "  -0.53773    2.7538     3.8162    -4.1612     0.7591    -2.8374\n",
      "  -6.4851    -3.3435     3.2703     2.759      2.6645     4.0013\n",
      "  13.381     -5.2907    -3.133      4.5374   -11.899     -6.716\n",
      "  -0.041939  -2.0879     3.0101    10.3        2.6835     2.7265\n",
      "   8.3018    -4.4563    14.43       3.9642    -4.8287    -5.648\n",
      "  -7.2597   -11.475     -2.6171     0.3325    14.454     -5.155\n",
      "   0.93722   -2.6187    -1.783      3.8711     1.4681    -6.705\n",
      "  -4.0953    -0.22536    9.444    -10.305     -0.13202   -2.5534\n",
      "   0.36113   -8.539      2.6755    -2.5872     2.8679     9.7515\n",
      "  -2.1221     0.82061  -10.319      1.1547    -6.5808     4.9236\n",
      "  -5.0744    -1.4781    -4.9195    -8.3767     9.5575    -3.685\n",
      "   2.1198    -0.17453    2.0831    -3.9546     1.0606     2.7273\n",
      "  -3.8842    -1.3651    -9.3881     8.7217     0.36486    6.9968\n",
      "   5.9808    -1.9575    -4.9097     0.17648    0.82424    3.5459\n",
      "   0.63201   -5.3636     0.18387    5.3658    -7.0217    -6.1577\n",
      "  12.318     -9.2032    -3.8502    -2.3115     3.8475     1.1169\n",
      " -19.406      1.3987     4.8752    -1.3337   -10.459    -10.679\n",
      "  -8.125      1.8708    -8.5214    -5.3364    -1.5602     1.5305\n",
      "   7.3319     1.4623    -4.2276     3.925     -3.6453    -8.9947\n",
      "  -9.9602     4.5267    -2.8874     3.9509     0.33015   -0.69045\n",
      "  -8.5682     5.6035     4.4303    -0.12195   -9.1766     7.1214\n",
      "  11.119    -13.376    -11.446     -4.6233     2.5693     1.0571\n",
      "  -3.7533     2.1846    11.634     -0.71756   -3.1931     9.6407\n",
      "  -0.97389   -9.5713     7.6816     6.216     -3.557      8.6693\n",
      "  -9.1699     6.9589    -3.7244    -1.4941   -11.266      3.6515\n",
      "  -6.93      13.741     -5.546      1.3314    -3.5116     4.2548\n",
      "  -4.1717     0.6486    13.1       -5.2464    -2.2528    -4.0023\n",
      "   0.80613    0.41242    5.1636     6.016    -11.018      2.5725\n",
      "  -5.4786     3.9834     1.7688     1.0016    -6.0102     0.75538\n",
      "  -3.0043     5.4985     5.7801    -7.1877    -0.80729    3.1282\n",
      "   0.39466    2.0783   -17.384     -7.25       3.3675    -2.8671\n",
      "  -4.7587     2.5144    -8.2825    13.736     -0.033851  -6.3096\n",
      " -17.447     -2.4643     5.0504     0.05487    8.3954    -7.3362\n",
      "   6.2495    -0.50043   13.246      4.4032    -5.1087     8.4952\n",
      " -11.692      1.8684    -4.0402    -7.5522    -3.1862     1.7126\n",
      "  -8.6044     7.4476     9.7846     4.0875     6.8338     7.4497\n",
      "   6.187     -5.592      4.6493     0.78633  -10.055      5.5839\n",
      "  -5.2267    -3.3644   -14.551      2.5234    -4.6496    -5.5413\n",
      "  -0.79268   -9.7865     2.4501    -0.26537  -15.947    -10.645\n",
      "   3.9164     1.9886     9.6709    -1.5746     1.333      1.628\n",
      "  -3.5716     2.7336     9.7687     1.9344     4.3976    -2.1922\n",
      "  -4.7053     0.61891    8.0963     7.9379   -11.57       4.3334  ], Shape: (300,)\n",
      "Word: love, Vector: [ 2.0565e+00 -3.2259e+00 -5.7364e+00 -6.1460e+00  1.5748e-01 -2.4284e+00\n",
      "  7.6580e+00  2.7064e+00 -2.2110e+00 -8.9990e-01  6.7584e+00 -2.6983e+00\n",
      " -7.6898e+00  2.4036e+00  7.9365e+00 -2.1374e+00 -1.7134e-01 -1.9848e+00\n",
      "  2.1000e+00  2.0230e+00 -1.1329e-01  3.7908e+00 -3.3405e+00 -8.5698e+00\n",
      "  3.6204e+00  9.6741e-01 -2.4264e+00  4.4687e+00  1.5334e+00  1.3886e+00\n",
      "  1.4789e+00 -4.5457e+00 -1.0838e+00 -1.9183e+00  3.3245e+00 -1.4215e-01\n",
      " -1.9783e+00  5.4134e-01  1.9844e+00 -7.2322e-02 -1.3614e+00  3.2423e+00\n",
      "  3.5776e+00 -2.6719e+00  1.0900e+00  4.6470e+00 -2.1616e+00 -3.8358e+00\n",
      "  4.1603e+00  4.6552e+00 -1.6725e+00 -4.4985e+00 -2.8982e-01 -4.5826e+00\n",
      " -4.6451e-02 -4.6299e-01  8.8783e-01  1.2574e+00  6.6601e+00  5.5503e+00\n",
      "  3.5401e+00  2.6721e-01  3.7113e+00 -4.0592e+00  9.4553e-01  5.9527e+00\n",
      " -4.1922e+00 -6.9438e+00  5.0857e+00  1.4050e+00 -2.4574e+00  7.3218e+00\n",
      " -2.1215e+00 -6.7814e-01 -2.2871e+00  1.8938e+00 -2.5832e+00  4.3815e+00\n",
      "  4.2151e+00 -3.6183e+00 -1.4703e+00 -1.3201e+00  2.6590e+00 -5.6996e+00\n",
      " -2.0867e-02  9.6205e-01  2.0081e+00  1.4750e+00 -7.7691e+00 -3.4898e+00\n",
      "  3.8443e+00  3.2584e+00 -6.3080e-01 -8.3240e+00 -3.6071e+00 -5.0766e+00\n",
      "  1.5934e+00 -2.2644e+00 -1.5554e+00 -2.9191e+00 -1.8064e-01  6.5120e-01\n",
      " -2.9984e-01  4.0194e+00  2.2026e-01  8.0274e-01 -1.7585e+00  1.8378e+00\n",
      " -3.2169e+00  2.6540e+00  2.3664e+00 -6.5790e+00  6.5348e-01 -1.3508e+00\n",
      "  2.1475e+00  3.2138e+00 -8.9947e+00 -7.6404e+00 -1.2826e+00 -1.0916e+00\n",
      " -4.8551e+00  2.3193e+00 -5.1803e+00  7.3055e-01 -1.5054e+00  3.5798e-01\n",
      "  4.8880e+00 -3.3836e+00  8.9427e-01 -2.6204e+00 -3.7773e+00  2.2107e+00\n",
      " -4.8519e+00 -9.8902e-01 -3.3669e+00  2.7894e+00 -5.0841e+00 -5.7924e-01\n",
      "  3.9221e+00 -2.8150e+00 -1.3585e+00  4.9767e+00  4.6014e+00  1.8252e+00\n",
      " -9.3133e-01 -4.7533e+00 -3.0548e+00  3.4432e-01 -3.3965e+00 -2.8556e-01\n",
      "  5.5976e-01 -2.4043e+00  2.4901e-02 -1.7460e+00 -2.4686e+00  5.2176e+00\n",
      " -7.7651e-02  4.1721e-01  9.3170e-01 -3.6655e+00  9.2885e-01 -5.1232e+00\n",
      " -1.4109e+00 -2.2130e+00  1.2593e-01  1.7002e-01  3.6620e-01 -3.3297e+00\n",
      " -2.8706e+00 -3.5471e+00  2.8726e+00 -1.9366e+00  2.2992e+00 -1.2327e+00\n",
      "  2.2837e+00 -4.9106e+00  3.7301e+00  5.5283e+00  4.2388e+00 -2.1416e+00\n",
      "  3.8090e+00 -2.8421e+00  4.9338e+00 -1.4347e+00 -1.7137e+00  3.0704e+00\n",
      "  3.1565e+00 -2.9751e+00  2.4606e+00  4.1388e+00  4.0442e-01  2.2151e+00\n",
      " -1.1582e+00 -1.2870e+00  1.3246e+00 -3.9480e+00 -3.0005e+00  3.3863e+00\n",
      " -2.7484e+00 -1.3876e+00  1.0230e+00 -2.6053e+00 -6.9901e+00 -2.2942e+00\n",
      " -6.8935e+00 -4.1983e+00  9.8276e-01  1.3395e+00 -4.8869e-02 -1.2287e+00\n",
      "  1.4470e+00 -7.9510e-01  4.5749e+00  6.5947e-01  2.2056e+00  2.6567e+00\n",
      " -4.8067e+00  4.3957e+00 -1.2664e+00 -1.5321e+00  4.6449e+00 -3.8600e-01\n",
      "  4.4860e-01  9.9003e+00 -1.8173e+00  1.5570e+00 -2.0185e+00 -2.5939e-01\n",
      "  7.0708e+00  2.1668e+00 -5.5965e+00 -3.6880e+00  1.0096e+00 -5.1063e+00\n",
      " -3.2259e-01  3.9952e-01 -5.6467e+00  5.2380e+00  1.7802e-01 -2.3491e-01\n",
      " -6.9355e-01 -3.3943e+00  1.0203e+00  1.3879e+00  3.7037e+00 -1.1950e-01\n",
      " -4.8030e+00 -3.3641e+00  3.7098e+00  3.9947e-01 -3.6545e+00  2.5716e+00\n",
      " -1.3741e+00 -1.6315e+00  2.0852e+00  2.8567e-01 -2.0594e+00  2.6534e+00\n",
      " -6.3363e+00  1.8357e+00 -1.4042e+00 -3.6166e-01  1.3117e+00 -9.2785e-01\n",
      "  3.6175e+00 -2.6949e+00 -2.2252e+00  5.5984e-02 -3.1778e+00  4.1249e-01\n",
      "  6.4716e+00 -1.3846e+00 -2.0004e+00 -4.4885e-01 -3.6592e+00  2.4708e+00\n",
      "  6.9389e+00 -4.7911e+00 -1.1108e+00  4.3447e+00 -3.4196e+00 -7.2039e-01\n",
      "  2.5732e+00 -3.7553e-01 -1.4460e+00 -6.7010e-01  1.0171e+00  2.8546e+00\n",
      " -4.2695e+00  1.4214e+00  1.5802e+00  1.7597e+00 -6.0806e-01 -6.6107e+00\n",
      "  9.3832e-03 -4.2763e+00 -5.0507e-01  5.0049e+00 -8.5312e+00 -1.4967e+00], Shape: (300,)\n",
      "Word: learning, Vector: [ 5.2415e-01 -1.1843e-01  9.9918e-01 -9.2592e-01  5.8506e-01  2.0744e+00\n",
      "  2.6959e+00  2.2769e+00 -4.9479e+00 -7.9734e-01  9.2866e+00  3.6112e+00\n",
      " -5.2253e+00  3.4473e+00  1.3158e+00 -4.6086e-01  4.7559e+00 -1.9305e-01\n",
      " -3.8121e+00  4.2875e+00 -3.2872e-01  1.8281e+00 -1.4754e+00 -1.9032e+00\n",
      " -1.7032e+00 -4.0926e+00 -1.8351e+00  1.0870e+00 -3.3411e+00  7.4851e+00\n",
      "  1.1742e+00 -3.0373e-01 -2.5485e+00 -1.9279e+00  1.6646e+00 -1.0174e+00\n",
      "  2.3636e+00  4.4008e-01  4.0041e+00 -3.6133e+00  7.6183e-01  2.2741e+00\n",
      " -1.4279e+00 -8.0863e-01 -3.6204e+00  2.2990e+00 -6.2229e-01 -3.9037e+00\n",
      "  2.9872e+00 -6.3003e-01  2.3043e+00  6.9002e-01 -1.2194e+00 -3.3751e+00\n",
      " -1.3489e+00  3.4724e+00 -1.3294e+00  1.4738e+00  2.9479e+00 -4.7675e+00\n",
      "  3.6388e+00  3.5650e+00  1.4891e+00  1.1954e+00  3.3699e+00  5.7875e+00\n",
      " -4.6507e+00 -4.6967e+00  2.6579e+00  1.8115e+00 -3.1754e+00  1.1248e+00\n",
      " -2.2645e+00  1.1069e+00  1.8203e-01 -2.2554e-01 -7.2056e+00  1.5426e+00\n",
      " -3.3017e+00 -4.0340e-01 -9.1280e+00 -5.8183e-01 -5.0528e-01  2.3604e+00\n",
      " -1.2982e+00  3.5190e+00 -9.1207e-01 -5.2398e+00  8.0220e-01 -1.9270e+00\n",
      "  7.9124e-01  2.0268e+00  4.1546e-01 -5.6516e+00 -2.8672e+00  1.6159e+00\n",
      " -1.9610e+00 -1.3406e+00  3.9821e+00  2.1241e+00  3.4758e+00  1.6072e+00\n",
      "  6.4524e+00  4.7497e+00 -1.8914e+00  4.8812e+00  3.9606e-01 -2.8332e+00\n",
      " -2.3677e+00  4.6346e-02  4.7779e+00 -2.1437e+00 -5.7385e+00  7.9065e-01\n",
      "  2.5260e+00  2.3132e+00 -5.7583e+00 -4.3681e-01 -2.5120e+00 -4.5710e+00\n",
      " -7.2244e+00  8.9461e-01 -1.5163e+00 -1.5839e+00  1.0655e+00 -3.7839e+00\n",
      "  2.3238e+00 -5.2761e+00  3.5663e+00 -4.4710e+00 -4.1136e+00  3.5428e-01\n",
      "  4.7834e+00  3.2816e-01 -1.1519e+00 -9.2081e-03 -6.5620e+00  9.0176e-01\n",
      "  5.4392e+00 -3.5219e-01  3.3274e+00  1.6719e+00 -2.8215e+00  1.0678e+00\n",
      "  1.7774e+00 -1.7107e+00 -6.0280e+00 -2.9113e+00 -2.4965e-01  2.4730e+00\n",
      "  3.5397e-01  2.2963e+00  3.5072e+00 -8.0545e-01 -2.0138e+00  9.0772e-01\n",
      "  3.8639e+00  4.7234e+00  7.2134e-01 -7.7882e+00 -1.7548e+00 -3.5279e-01\n",
      " -2.1562e-01  1.5422e+00 -1.9912e+00 -3.6084e+00  3.5919e-01  1.5846e+00\n",
      " -1.2086e+00 -3.3831e-01 -7.9229e-01  1.2602e+00  1.9991e-01  2.0949e+00\n",
      "  7.3416e+00  2.0928e+00  4.4233e+00 -3.7646e+00 -2.8372e+00 -4.6271e+00\n",
      " -1.1910e+00  1.9580e+00  6.1960e+00 -2.6755e+00 -2.5112e+00  2.1277e+00\n",
      " -4.9013e-01 -6.3878e+00  1.1589e+00  1.4311e+00 -1.3822e+00 -2.1891e+00\n",
      " -5.0577e-01 -1.5217e+00  3.7201e+00 -4.8566e+00 -4.0714e+00 -4.7532e-01\n",
      " -6.0586e-01 -2.4879e-01  2.0676e-01 -8.7559e-03 -2.4218e+00 -4.1957e+00\n",
      "  5.0531e+00 -1.8014e+00 -4.9046e+00  1.9647e+00 -3.1957e-01 -2.2874e+00\n",
      "  4.1063e+00  3.5684e-02  4.6252e-01  1.2987e+00 -4.6031e+00  2.0689e+00\n",
      "  3.3452e+00  2.7237e-02 -6.0581e-01  2.5498e+00 -1.8716e+00  3.7908e+00\n",
      " -3.2418e+00  2.2543e+00  2.5722e+00 -1.9206e-01  1.7479e+00 -1.2751e+00\n",
      "  4.1059e+00  3.6211e+00  6.3604e+00 -2.9154e+00 -2.0393e+00  1.4650e+00\n",
      "  1.4673e+00  1.4110e+00  1.2549e+00 -1.0626e+00 -4.9468e-01  2.5788e+00\n",
      " -6.7368e+00 -1.2697e+00 -1.4319e+00  2.0562e+00  1.5537e+00  5.1767e+00\n",
      " -4.7995e+00 -8.8018e-01  1.2586e-01  6.3989e-01 -1.9193e+00  4.5036e+00\n",
      " -4.7457e+00  1.0940e+00 -3.0832e+00  7.1560e-01  3.2145e+00 -3.7101e+00\n",
      " -6.9132e-02  2.6534e+00 -2.3662e+00  6.8079e+00  6.3903e+00 -2.8253e-01\n",
      "  1.7868e+00 -1.8786e+00 -2.6506e+00  2.0292e+00 -3.0831e+00 -2.5823e+00\n",
      "  1.5028e-01 -3.8987e+00  1.2872e+00 -4.5131e+00 -2.4275e+00 -1.0103e+00\n",
      "  7.7083e-01 -3.4718e+00 -6.1662e-01  9.6295e-01  2.8688e+00 -8.8213e-01\n",
      "  2.8095e+00  3.6612e-01 -8.3534e-01  2.9433e-01  4.6173e+00  2.8232e+00\n",
      " -2.3742e+00 -1.8487e+00  9.8842e-01 -1.1012e+00 -1.5634e+00 -3.1777e+00\n",
      "  1.6411e+00 -4.3231e+00  1.3725e+00 -1.7378e+00 -4.2054e+00  4.2028e+00], Shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f\"Word: {token}, Vector: {token.vector}, Shape: {token.vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with Flair and Glove now (You will find how to do [here](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"I\"\n",
      "tensor([-0.0465,  0.6197,  0.5665, -0.4658, -1.1890,  0.4460,  0.0660,  0.3191,\n",
      "         0.1468, -0.2212,  0.7924,  0.2991,  0.1607,  0.0253,  0.1868, -0.3100,\n",
      "        -0.2811,  0.6051, -1.0654,  0.5248,  0.0642,  1.0358, -0.4078, -0.3801,\n",
      "         0.3080,  0.5996, -0.2699, -0.7603,  0.9422, -0.4692, -0.1828,  0.9065,\n",
      "         0.7967,  0.2482,  0.2571,  0.6232, -0.4477,  0.6536,  0.7690, -0.5123,\n",
      "        -0.4433, -0.2187,  0.3837, -1.1483, -0.9440, -0.1506,  0.3001, -0.5781,\n",
      "         0.2017, -1.6591, -0.0792,  0.0264,  0.2205,  0.9971, -0.5754, -2.7266,\n",
      "         0.3145,  0.7052,  1.4381,  0.9913,  0.1398,  1.3474, -1.1753,  0.0040,\n",
      "         1.0298,  0.0646,  0.9089,  0.8287, -0.4700, -0.1058,  0.5916, -0.4221,\n",
      "         0.5733, -0.5411,  0.1077,  0.3978, -0.0487,  0.0646, -0.6144, -0.2860,\n",
      "         0.5067, -0.4976, -0.8157,  0.1641, -1.9630, -0.2669, -0.3759, -0.9585,\n",
      "        -0.8584, -0.7158, -0.3234, -0.4312,  0.4139,  0.2837, -0.7093,  0.1500,\n",
      "        -0.2154, -0.3762, -0.0325,  0.8062])\n",
      "Token[1]: \"love\"\n",
      "tensor([ 2.5975e-01,  5.5833e-01,  5.7986e-01, -2.1361e-01,  1.3084e-01,\n",
      "         9.4385e-01, -4.2817e-01, -3.7420e-01, -9.4499e-02, -4.3344e-01,\n",
      "        -2.0937e-01,  3.4702e-01,  8.2516e-02,  7.9735e-01,  1.6606e-01,\n",
      "        -2.6878e-01,  5.8830e-01,  6.7397e-01, -4.9965e-01,  1.4764e+00,\n",
      "         5.5261e-01,  2.5295e-02, -1.6068e-01, -1.3878e-01,  4.8686e-01,\n",
      "         1.1420e+00,  5.6195e-02, -7.3306e-01,  8.6932e-01, -3.5892e-01,\n",
      "        -5.1877e-01,  9.0402e-01,  4.9249e-01, -1.4915e-01,  4.8493e-02,\n",
      "         2.6096e-01,  1.1352e-01,  4.1275e-01,  5.3803e-01, -4.4950e-01,\n",
      "         8.5733e-02,  9.1184e-02,  5.0177e-03, -3.4645e-01, -1.1058e-01,\n",
      "        -2.2235e-01, -6.5290e-01, -5.1838e-02,  5.3791e-01, -8.1040e-01,\n",
      "        -1.8253e-01,  2.4194e-01,  5.4855e-01,  8.7731e-01,  2.2165e-01,\n",
      "        -2.7124e+00,  4.9405e-01,  4.4703e-01,  5.5882e-01,  2.6076e-01,\n",
      "         2.3760e-01,  1.0668e+00, -5.6971e-01, -6.4960e-01,  3.3511e-01,\n",
      "         3.4609e-01,  1.1033e+00,  8.5261e-02,  2.4847e-02, -4.5453e-01,\n",
      "         7.7012e-02,  2.1321e-01,  1.0444e-01,  6.7157e-02, -3.4261e-01,\n",
      "         8.5534e-01,  1.3361e-01, -4.3296e-01, -5.6726e-01, -2.1348e-01,\n",
      "        -3.3277e-01,  3.4351e-01,  3.2164e-01,  4.4527e-01, -1.3208e+00,\n",
      "        -1.3270e-01, -7.0820e-01, -4.8472e-01, -6.9396e-01, -2.6080e-01,\n",
      "        -4.7099e-01, -5.7492e-02,  9.3587e-02,  4.0006e-01, -4.3419e-01,\n",
      "        -2.7364e-01, -7.7017e-01, -8.4028e-01, -1.5620e-03,  6.2223e-01])\n",
      "Token[2]: \"learning\"\n",
      "tensor([ 0.6481,  0.6988, -0.3995,  0.7763, -0.1313,  0.2024, -0.3340, -0.0067,\n",
      "         0.0617,  0.1885, -0.1056, -0.3132, -0.0825, -0.0805,  0.3858, -0.1030,\n",
      "         0.0494,  0.1722, -0.5908,  0.7707, -1.2768, -0.2519,  0.2195, -0.2018,\n",
      "        -0.3058, -0.1852,  0.0109, -0.0753, -0.3473,  0.6200, -0.9970,  1.0516,\n",
      "        -0.4207, -0.3963,  0.3261, -0.4006, -0.4646,  0.6990,  0.2957, -0.3531,\n",
      "        -0.5907,  0.2900, -0.2573, -0.1317, -0.6980,  0.4982,  0.4150,  0.1487,\n",
      "         0.0833, -0.4354, -0.0940, -0.3543,  0.0150,  0.6359,  0.5456, -1.8439,\n",
      "         0.7884, -0.1984,  1.5707,  0.2599,  0.2087,  0.7521, -0.0855, -0.7072,\n",
      "         0.0941,  0.4448,  0.0878, -0.3478,  0.5715,  0.1866, -0.2943,  0.4293,\n",
      "         0.2839, -0.6161, -0.3411,  0.5819, -0.1639, -0.0082, -0.2716, -0.2711,\n",
      "        -0.2147,  0.3738, -0.5352, -0.0609, -1.6317,  0.8514,  0.0560, -0.5386,\n",
      "        -0.5838, -0.1961, -0.3394, -0.3141,  0.2300,  0.3269,  0.0430, -0.0375,\n",
      "        -0.0921, -1.0734,  0.8924,  0.4178])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "from flair.data import Sentence\n",
    "# Embed with Flair\n",
    "text = \"I love learning\"\n",
    "\n",
    "\n",
    "# init embedding\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "# create sentence.\n",
    "sentence = Sentence(text)\n",
    "\n",
    "# embed a sentence using glove.\n",
    "glove_embedding.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of each word's vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Token[0]: \"I\", shape of word vector: torch.Size([100])\n",
      "Word: Token[1]: \"love\", shape of word vector: torch.Size([100])\n",
      "Word: Token[2]: \"learning\", shape of word vector: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(f\"Word: {token}, shape of word vector: {token.embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your text is now embedded, your model will be able to understand it, yeah!\n",
    "\n",
    "## Maths on text\n",
    "\n",
    "Since the words are embedded into vectors we can now apply mathematical methods on them.\n",
    "\n",
    "### Average vector\n",
    "\n",
    "For example we could build the average vector for a text by using NumPy! This is a straightforward way to build one single representation for a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Vector: [-1.39259255e+00  4.43511915e+00 -2.87938738e+00 -1.46659994e+00\n",
      " -2.49136314e-01  3.17635000e-01  3.21812510e-01  5.20731592e+00\n",
      " -1.49731112e+00  2.29289985e+00  8.17859936e+00  2.53242493e+00\n",
      " -3.80331492e+00 -1.94080308e-01  2.12294507e+00  5.62517643e-01\n",
      "  3.66180110e+00 -3.78243327e+00 -5.52819538e+00 -2.37863755e+00\n",
      "  3.47129107e+00 -9.34405386e-01 -2.44463801e+00 -2.58583379e+00\n",
      " -2.49281645e+00 -3.74811745e+00 -2.65185761e+00 -2.05591232e-01\n",
      " -3.03205299e+00  9.44486260e-02  1.37055504e+00 -8.00651193e-01\n",
      " -2.86574006e+00  1.37557483e+00  8.98500085e-02 -1.28024876e+00\n",
      " -7.56257534e-01 -1.58593631e+00  5.12072420e+00  1.32494879e+00\n",
      " -2.21358752e+00  2.15078497e+00  1.60805118e+00 -3.99998128e-02\n",
      " -1.08419502e+00  9.56671178e-01  2.91092491e+00 -4.15709496e+00\n",
      " -1.64297867e+00  1.94995987e+00 -2.10612342e-02  2.99287438e-02\n",
      "  2.17419982e-01 -6.81680346e+00 -2.71650863e+00  2.75750458e-03\n",
      " -1.69070005e-01  1.74406245e-01  1.23772871e+00 -9.59155202e-01\n",
      "  3.64199209e+00 -5.54854989e-01 -2.11917138e+00  1.52189955e-01\n",
      "  1.09138238e+00  1.57937884e+00 -2.63529611e+00 -4.77612448e+00\n",
      "  2.01567864e+00  4.85690117e+00 -7.94467688e-01  2.94771242e+00\n",
      " -3.54596198e-01 -4.37968820e-01 -5.95188737e-01  3.90962029e+00\n",
      " -3.62537098e+00  8.93907368e-01 -5.84652472e+00 -5.49675894e+00\n",
      " -6.38753223e+00 -4.18284893e-01  4.95839834e+00  1.67524993e-01\n",
      "  2.05860376e+00 -8.78972411e-01  1.06125236e+00 -5.77971339e-01\n",
      "  4.62316513e+00 -2.67813754e+00 -8.29950094e-01 -2.41648149e+00\n",
      "  3.50101256e+00 -6.45668745e+00  2.36415720e+00 -3.18415117e+00\n",
      "  5.64917505e-01 -2.04227448e+00 -1.38119996e-01  8.93087089e-02\n",
      "  3.05852413e-01  2.92151737e+00  1.62633884e+00 -5.74025512e-02\n",
      " -3.57008266e+00  4.14367485e+00 -2.67925715e+00 -3.20531368e+00\n",
      " -2.40072346e+00 -1.95755756e+00  3.13712388e-01  1.65576744e+00\n",
      " -5.23747027e-01  1.39405632e+00  8.67812514e-01  5.25249958e-01\n",
      " -3.96169996e+00 -3.03460360e+00  2.34248972e+00 -1.37843370e+00\n",
      " -3.98261857e+00 -2.39283109e+00 -1.87087250e+00  3.04894257e+00\n",
      " -2.80111194e-01 -3.56902266e+00  1.68735003e+00 -1.48825622e+00\n",
      "  1.64286757e+00 -3.42867464e-01 -8.96051228e-01  8.67793441e-01\n",
      "  1.79496133e+00 -3.04881263e+00 -1.52324390e+00  9.73850012e-01\n",
      " -3.44703031e+00 -1.84281230e-01  6.42865038e+00 -2.75308228e+00\n",
      " -4.59500408e+00 -9.76713777e-01  2.37400484e+00  1.21148372e+00\n",
      " -5.10531187e-01 -1.19925141e-02  1.05392516e-01  8.03014040e-01\n",
      " -1.56667233e+00  9.25361156e-01 -3.17005014e+00  5.55269289e+00\n",
      " -3.03376406e-01  8.09092641e-01  6.81092501e-01  1.34615135e+00\n",
      "  6.39607906e+00  2.03784895e+00 -3.24278879e+00 -5.79262495e-01\n",
      " -3.32134652e+00 -3.33835769e+00 -2.35975504e+00  2.04277992e+00\n",
      " -3.39158249e+00 -1.49768758e+00 -7.26635790e+00  1.80704367e+00\n",
      " -5.33117473e-01 -3.85887265e+00  2.09221244e+00 -1.63368762e-01\n",
      "  2.00302863e+00  3.98862529e+00  7.43587160e+00 -2.73484969e+00\n",
      " -2.14363980e+00 -3.56408834e+00 -1.45781004e+00 -4.26498747e+00\n",
      " -6.72173738e-01  1.34940374e+00  2.95524979e+00 -4.45724487e+00\n",
      " -1.03083491e+00  1.25852871e+00 -1.57553625e+00 -4.47630024e+00\n",
      "  4.82467556e+00  4.98346233e+00 -1.35972393e+00 -1.44926280e-01\n",
      " -3.48608780e+00 -1.01970625e+00 -1.20381236e+00  3.33604002e+00\n",
      " -6.52281284e+00  1.33125797e-01 -2.35725355e+00  1.52405763e+00\n",
      " -4.48644018e+00 -8.84902537e-01 -2.72900867e+00 -2.41130757e+00\n",
      "  8.63825083e-01  1.05834746e+00 -1.40079629e+00 -5.84456265e-01\n",
      "  1.55911855e-02 -2.99991280e-01  2.30085135e+00  6.25677466e-01\n",
      "  4.52906191e-01  5.02688742e+00  8.12752604e-01  2.13057137e+00\n",
      "  2.23315001e+00 -3.03002906e+00 -3.06367850e+00  6.76461220e-01\n",
      " -4.12710190e+00 -1.36728859e+00  2.70650029e-01  1.12949991e+00\n",
      "  1.69486105e+00 -3.91647482e+00 -7.60647655e-01  2.19374371e+00\n",
      "  2.86284113e+00  1.68913752e-01 -1.30756247e+00 -5.19603109e+00\n",
      " -1.13866115e+00  1.50545001e+00  2.64650047e-01  9.50621247e-01\n",
      " -1.28909516e+00  6.05350018e-01 -6.42306447e-01 -2.23910046e+00\n",
      " -5.52760029e+00  1.09819627e+00  2.06353760e+00  3.38694406e+00\n",
      " -1.00234246e+00 -4.11985993e-01 -1.41798997e+00  1.50525880e+00\n",
      "  4.34544134e+00  3.09045768e+00 -4.37305003e-01 -3.50357443e-01\n",
      " -5.46692514e+00 -2.34907150e+00 -3.84099990e-01 -3.66651750e+00\n",
      "  2.68497491e+00  1.02847135e+00 -9.47701752e-01  1.66209888e+00\n",
      "  5.71122587e-01  5.84746790e+00  3.77182460e+00  4.36114454e+00\n",
      "  2.17956138e+00 -8.99991274e-01  2.02220130e+00  3.44404650e+00\n",
      " -5.80329657e+00 -2.03847122e+00  1.16590798e+00 -1.33728254e+00\n",
      " -3.84236503e+00 -1.65016150e+00 -1.12764370e+00 -2.68441248e+00\n",
      "  3.71265054e-01 -7.84499943e-02 -3.75754976e+00  2.35947514e+00\n",
      " -2.56167865e+00 -2.66591144e+00 -8.89123380e-02  1.42465889e+00\n",
      "  6.40080881e+00 -3.22685432e+00  2.36551499e+00  3.03806019e+00\n",
      " -2.16833520e+00  2.54768634e+00  2.66416883e+00 -9.70258772e-01\n",
      "  8.49951267e-01  2.54588103e+00 -8.18835020e-01  1.38735127e+00\n",
      "  2.76343179e+00  1.12776268e+00 -4.59969234e+00  5.23336411e+00]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "text = \"I want to be a famous data scientist\"\n",
    "\n",
    "# Apply a spacy model on the text\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "tokens = nlp(text)\n",
    "\n",
    "# Get all word vectors into a list\n",
    "vector_list = []\n",
    "for token in tokens:\n",
    "    vector_list.append(token.vector)\n",
    "# print(vector_list)\n",
    "# Compute and display the average vector of the text\n",
    "average_text_vector = np.mean(vector_list, axis=0)\n",
    "print(f\"Average Vector: {average_text_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarity\n",
    "\n",
    "We can also compute the similarity between two words by using distance measures (e.g. [cosine distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html), [euclidean distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html)...). These measures will calculate the distance between word embeddings in the vector space.\n",
    "\n",
    "#### Let's practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4687861204147339 39.738563537597656\n",
      "0.46193528175354004 44.84297561645508\n",
      "1.0110153406858444 100.74956512451172\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import spacy\n",
    "from scipy.spatial import distance\n",
    "\n",
    "word1 = \"computer\"\n",
    "word2 = \"keyboard\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "token1 = nlp(word1)\n",
    "token2 = nlp(word2)\n",
    "\n",
    "# Get the vector for both words through your favorite model\n",
    "vector_1 = token1.vector\n",
    "vector_2 = token2.vector\n",
    "# Compute the cosine and the euclidean distance between both words\n",
    "cosine_dist = distance.cosine(vector_1, vector_2)\n",
    "euclidean_dist = distance.euclidean(vector_1, vector_2)\n",
    "print(cosine_dist, euclidean_dist)\n",
    "\n",
    "\n",
    "# Try with other pairs of words for comparing the results\n",
    "\n",
    "word1 = \"football\"\n",
    "word2 = \"tennis\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "token1 = nlp(word1)\n",
    "token2 = nlp(word2)\n",
    "\n",
    "# Get the vector for both words through your favorite model\n",
    "vector_1 = token1.vector\n",
    "vector_2 = token2.vector\n",
    "# Compute the cosine and the euclidean distance between both words\n",
    "cosine_dist = distance.cosine(vector_1, vector_2)\n",
    "euclidean_dist = distance.euclidean(vector_1, vector_2)\n",
    "print(cosine_dist, euclidean_dist)\n",
    "\n",
    "\n",
    "# Try with other pairs of words for comparing the results\n",
    "\n",
    "word1 = \"eat\"\n",
    "word2 = \"ball\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "token1 = nlp(word1)\n",
    "token2 = nlp(word2)\n",
    "\n",
    "# Get the vector for both words through your favorite model\n",
    "vector_1 = token1.vector\n",
    "vector_2 = token2.vector\n",
    "# Compute the cosine and the euclidean distance between both words\n",
    "cosine_dist = distance.cosine(vector_1, vector_2)\n",
    "euclidean_dist = distance.euclidean(vector_1, vector_2)\n",
    "print(cosine_dist, euclidean_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack embeddings\n",
    "\n",
    "The previous embeddings are good, but if you want something even better, you can \"stack\" these embeddings to create a bigger vector. It gives better results but will also require more computation power.\n",
    "\n",
    "[Here is a super clear and understandable guide](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md) to get it done. (by the Flair's team)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-03 16:41:07,766 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/news-forward-0.4.1.pt not found in cache, downloading to C:\\Users\\grego\\AppData\\Local\\Temp\\tmpcsaf5kj2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69.7M/69.7M [00:05<00:00, 12.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-03 16:41:13,965 copying C:\\Users\\grego\\AppData\\Local\\Temp\\tmpcsaf5kj2 to cache at C:\\Users\\grego\\.flair\\embeddings\\news-forward-0.4.1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-03 16:41:14,016 removing temp file C:\\Users\\grego\\AppData\\Local\\Temp\\tmpcsaf5kj2\n",
      "2024-01-03 16:41:15,066 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/news-backward-0.4.1.pt not found in cache, downloading to C:\\Users\\grego\\AppData\\Local\\Temp\\tmpzogwiwzj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69.7M/69.7M [00:05<00:00, 14.2MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-03 16:41:20,801 copying C:\\Users\\grego\\AppData\\Local\\Temp\\tmpzogwiwzj to cache at C:\\Users\\grego\\.flair\\embeddings\\news-backward-0.4.1.pt\n",
      "2024-01-03 16:41:20,852 removing temp file C:\\Users\\grego\\AppData\\Local\\Temp\\tmpzogwiwzj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[5]: \"The grass is green.\"\n",
      "Token[0]: \"The\"\n",
      "tensor([-0.0382, -0.2449,  0.7281,  ..., -0.0065, -0.0053,  0.0090])\n",
      "Token[1]: \"grass\"\n",
      "tensor([-0.8135,  0.9404, -0.2405,  ...,  0.0354, -0.0255, -0.0143])\n",
      "Token[2]: \"is\"\n",
      "tensor([-5.4264e-01,  4.1476e-01,  1.0322e+00,  ..., -5.3690e-04,\n",
      "        -9.6750e-03, -2.7541e-02])\n",
      "Token[3]: \"green\"\n",
      "tensor([-0.6791,  0.3491, -0.2398,  ..., -0.0007, -0.1333,  0.0161])\n",
      "Token[4]: \".\"\n",
      "tensor([-0.3398,  0.2094,  0.4635,  ...,  0.0005, -0.0177,  0.0032])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
    "\n",
    "# create a StackedEmbedding object that combines glove and forward/backward flair embeddings\n",
    "stacked_embeddings = StackedEmbeddings([WordEmbeddings('glove'), FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward')])\n",
    "\n",
    "sentence = Sentence('The grass is green.')\n",
    "\n",
    "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
    "stacked_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources\n",
    "* [Why do we use word embeddings in NLP?](https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2)\n",
    "* [More details on what word embeddings are exactly?](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1ea57bea45b518cf48c15b34c69ddf5a63c6b38f630bac26c11b544b5b7ce7d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
